{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "#\n",
    "# imports and definitions\n",
    "#\n",
    "#\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import networkx as nx\n",
    "import itertools as it\n",
    "import random as rd\n",
    "import os.path\n",
    "from collections import defaultdict\n",
    "import time\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "\n",
    "def rnd_walk_matrix(A, r, alpha, num_nodes):\n",
    "    \n",
    "    n = num_nodes\n",
    "    M = normalize(A, norm='l1', axis=0)                                 # column wise normalized MArkov matrix\n",
    "    factor = float((1-alpha)/num_nodes)\n",
    "#     print(factor)\n",
    "    E = np.multiply(factor,np.ones([num_nodes,num_nodes]))              # prepare 2nd scaling term\n",
    "    M2 = np.multiply(alpha,M) + E                                       # mixture of Markov chains\n",
    "    del M\n",
    "    del E\n",
    "    \n",
    "    U = np.identity(n,dtype=int) \n",
    "    H = (1-r)*M2\n",
    "    H1 = np.subtract(U,H)\n",
    "    del U\n",
    "    del M2\n",
    "    del H    \n",
    "\n",
    "    W = r*np.linalg.inv(H1)                                             # calculate random-walk matrix\n",
    "    del H1\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# defining the input: parameters, seed sets and network\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "rlist = [.9] # restart probabiltiy \n",
    "alpha = float(1)\n",
    "r=.9\n",
    "\n",
    "#nf_literature_proteins = ['4763', '3845',  '4893', '3265', '22882', '5609', '5594', '161742', '5291', '207', '2475']\n",
    "\n",
    "\n",
    "diseasegenes_dict={} # the dictionary where you keep your diseas genes \n",
    "#diseasegenes_dict['NF']=['4763', '3845',  '4893', '3265', '22882', '5609', '5594', '161742', '5291', '207', '2475']\n",
    "#diseasegenes_dict['ALPS']=['355', '356', '841', '836', '839', '840']\n",
    "#diseasegenes_dict['HEM1']=['3071', '207', '10006' ,'8936', '60', '253260','2475','79109','64223']\n",
    "#diseasegenes_dict['WDR1']=['9948', '60', '1072','818', '11034']\n",
    "#diseasegenes_dict['TTC7A']=['57217', '855252', '5297', '84668', '23209', '3690']\n",
    "\n",
    "'''\n",
    "diseasegenes_dict['PTPN2']=['5771']\n",
    "diseasegenes_dict['FERMT1']=['55612']\n",
    "diseasegenes_dict['RPIA']=['22934']\n",
    "diseasegenes_dict['RAG1']=['5896']\n",
    "diseasegenes_dict['BLM']=['641']\n",
    "\n",
    "\n",
    "diseasegenes_dict['CTLA4']=['1493']\n",
    "diseasegenes_dict['AIRE']=['326']\n",
    "diseasegenes_dict['CFTR']=['1080']\n",
    "diseasegenes_dict['RPSA']=['3921']\n",
    "diseasegenes_dict['ADA2']=['51816']\n",
    "diseasegenes_dict['GATA2']=['2624']\n",
    "diseasegenes_dict['HTT']=['3064']\n",
    "diseasegenes_dict['UBE3A']=['7337']\n",
    "diseasegenes_dict['TYR']=['7299']\n",
    "diseasegenes_dict['SERPINEA1']=['5265']\n",
    "'''\n",
    "\n",
    "diseasegenes_dict['HEM1']=['3071','60','10006','8936','10787','23191','26999','55845','253260','2475','79109','64223','55615','79899','207']\n",
    "\n",
    "# this will be your network\n",
    "networkfile= '/Users/Iva/Documents/project/data/monster_clean.txt'\n",
    "#this will be your dictionary of seed genes (the nodes that you will start the random walk from)\n",
    "\n",
    "seedgroups=diseasegenes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is NOT connected! Giant component taken.\n",
      "lcc network: #nodes=18818 , # edges=483257 \n",
      "calculating adjacency matrix\n",
      "Inversion done, with r== 0.9\n",
      "rnd walk computing time: 416.73\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "# creating the random walk matrix of the network\n",
    "#\n",
    "#\n",
    "\n",
    "G0 = nx.read_edgelist(networkfile)\n",
    "#G = max(nx.connected_component_subgraphs(G0), key=len)\n",
    "G = max((G0.subgraph (c) for c in nx.connected_components(G0)), key=len)\n",
    "\n",
    "if G.number_of_nodes() < G0.number_of_nodes():\n",
    "    print('Network is NOT connected! Giant component taken.')\n",
    "else:\n",
    "    print('Network is connected.')\n",
    "\n",
    "print('lcc network: #nodes=%s , # edges=%s ' %(G.number_of_nodes(),G.number_of_edges()))\n",
    "\n",
    "# ###########################################################################\n",
    "#                                                                           #\n",
    "#    CALCULATES MATRIX INVERSION FOR GIVEN PARAMETERS                       #\n",
    "#                                                                           #\n",
    "print('calculating adjacency matrix')                                                                            #\n",
    "num_nodes = G.number_of_nodes()                                             #\n",
    "A = nx.adjacency_matrix(G, sorted(G.nodes()))                               #\n",
    "                                                                       #\n",
    "#################################################################           #\n",
    "#                                                                           #\n",
    "#    INVERT MARKOV MATRIX & GENERATE RW MATRIX                              #\n",
    "#                                                                           #\n",
    "###################                                                         #\n",
    "t0 = time.time()\n",
    "W = rnd_walk_matrix(A, r, alpha, num_nodes)                                 #\n",
    "                                                                        #\n",
    "print('Inversion done, with r== %s' %(r))                                   #\n",
    "#################################################################           #\n",
    "#                                                                           #\n",
    "#    GENERATE DICT FOR NODE-LABELS TO INTEGERS                              #\n",
    "#                                                                           #\n",
    "###################                                                         #\n",
    "                                                                        #\n",
    "d_idx_entz = {}                                                             #\n",
    "cc = 0                                                                      #\n",
    "for entz in sorted(G.nodes()):                                              #\n",
    "    d_idx_entz[cc] = entz                                                   #\n",
    "    cc += 1                                                                 #\n",
    "d_entz_idx = dict((y,x) for x,y in d_idx_entz.items())                      #\n",
    "print('rnd walk computing time: %.2f' %float(time.time()-t0))               #\n",
    "                                                                        #\n",
    "#############################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All genes in network.\n",
      "number of genes in set: 15\n",
      "number of set genes on network: 15\n",
      "Size of initial set: 15 seed genes\n",
      "check for sum of initial p-vec\n",
      "15.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\n",
    "#\n",
    "# going through each seed node and making the visit probability dicitonary\n",
    "#\n",
    "#\n",
    "\n",
    "visit_prob_dictionaries={}  ##name of dictionaries inside is gene names\n",
    "for thegroup, l_ent in seedgroups.items(): \n",
    "    seeds= thegroup\n",
    "    if (set(l_ent).issubset(set(G.nodes()))):\n",
    "        print('All genes in network.')\n",
    "        l_ent2cont = l_ent\n",
    "    else:\n",
    "        l_ent2cont = []\n",
    "        for g in l_ent:\n",
    "            if g in G:\n",
    "                l_ent2cont.append(g)\n",
    "            else:\n",
    "                print('Genes %s is not part of the network and will be neglected!' %g)\n",
    "    print('number of genes in set: %s' %len(l_ent))\n",
    "    print('number of set genes on network: %s' %len(l_ent2cont))\n",
    "\n",
    "    # make list of indices\n",
    "    nodeset = []\n",
    "    for s_ent in l_ent2cont:\n",
    "        nodeset.append(d_entz_idx[s_ent])\n",
    "\n",
    "    normprob = len(nodeset)\n",
    "    print('Size of initial set: %s seed genes' %normprob)\n",
    "    p0 = np.zeros(G.number_of_nodes())\n",
    "    # generate start vector\n",
    "    for n in range(len(p0)):\n",
    "        if n in nodeset:\n",
    "            p0[n] = 1.\n",
    "    print('check for sum of initial p-vec')\n",
    "    print(np.sum(p0))\n",
    "    pinf =  np.array(W.dot(p0))\n",
    "\n",
    "    # DICT WITH NODE ID AND PVIS\n",
    "    d_n_p = {}\n",
    "    i = 0\n",
    "    for x in pinf[0]:\n",
    "        #     print(i,x)\n",
    "        d_n_p[d_idx_entz[i]] = x/normprob\n",
    "        i += 1\n",
    "\n",
    "    # write file\n",
    "\n",
    "    pcum = 0\n",
    "    i=0\n",
    "    sorted_d_n_p= sorted(d_n_p.items(), key = lambda x: x[1], reverse = True)\n",
    "    visit_prob_dictionaries[thegroup]=sorted_d_n_p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['HEM1'])\n"
     ]
    }
   ],
   "source": [
    "len (visit_prob_dictionaries)\n",
    "print (visit_prob_dictionaries.keys())\n",
    "\n",
    "#nf_rw = visit_prob_dictionaries[\"NF\"]\n",
    "#alps_rw = visit_prob_dictionaries[\"ALPS\"]\n",
    "#hem1_rw = visit_prob_dictionaries[\"HEM1\"]\n",
    "#wdr1_rw = visit_prob_dictionaries[\"WDR1\"]\n",
    "#ttc7a_rw = visit_prob_dictionaries[\"TTC7A\"]\n",
    "#rictor_rw = visit_prob_dictionaries['RICTOR']\n",
    "'''\n",
    "ptpn2_rw = visit_prob_dictionaries['PTPN2']\n",
    "fermit1_rw = visit_prob_dictionaries['FERMT1']\n",
    "rpia_rw=visit_prob_dictionaries['RPIA']\n",
    "rag1_rw=visit_prob_dictionaries['RAG1']\n",
    "blm_rw=visit_prob_dictionaries['BLM']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ctla4_rw = visit_prob_dictionaries['CTLA4']\n",
    "aire_rw = visit_prob_dictionaries['AIRE']\n",
    "cftr_rw = visit_prob_dictionaries['CFTR']\n",
    "rpsa_rw = visit_prob_dictionaries['RPSA']\n",
    "ada2_rw = visit_prob_dictionaries['ADA2']\n",
    "gata2_rw = visit_prob_dictionaries['GATA2']\n",
    "htt_rw = visit_prob_dictionaries['HTT']\n",
    "ube3a_rw = visit_prob_dictionaries['UBE3A']\n",
    "tyr_rw = visit_prob_dictionaries['TYR']\n",
    "serpinea1_rw = visit_prob_dictionaries['SERPINEA1']\n",
    "\n",
    "'''\n",
    "\n",
    "hem1 = visit_prob_dictionaries['HEM1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nf2 = open (\"aire_rw.txt\", \"w\")\\nfor j in aire_rw:   \\n    f2.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf2.close()\\n\\nf3 = open (\"cftr_rw.txt\", \"w\")\\nfor j in cftr_rw:   \\n    f3.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf3.close()\\n\\nf4 = open (\"rpsa_rw.txt\", \"w\")\\nfor j in rpsa_rw:   \\n    f4.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf4.close()\\n\\nf5 = open (\"ada2_rw.txt\", \"w\")\\nfor j in ada2_rw:   \\n    f5.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf5.close()\\n\\nf6 = open (\"gata2_rw.txt\", \"w\")\\nfor j in gata2_rw:   \\n    f6.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf6.close()\\n\\nf7 = open (\"htt_rw.txt\", \"w\")\\nfor j in htt_rw:   \\n    f7.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf7.close()\\n\\nf8 = open (\"ube3a_rw.txt\", \"w\")\\nfor j in ube3a_rw:   \\n    f8.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf8.close()\\n\\nf9 = open (\"tyr_rw.txt\", \"w\")\\nfor j in tyr_rw:   \\n    f9.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf9.close()\\n\\nf10 = open (\"serpinea1_rw.txt\", \"w\")\\nfor j in serpinea1_rw:   \\n    f10.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf10.close()\\n\\n\\n\\n\\n\\nf1 = open (\"rictor_rw.txt\", \"w\")\\nfor j in rictor_rw:   \\n    f1.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\\nf1.close()\\n\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "   \n",
    "f1 = open (\"hem1_literature_all_rw.txt\", \"w\")\n",
    "for j in hem1:   \n",
    "    f1.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f1.close()\n",
    "\n",
    "\n",
    "'''\n",
    "f2 = open (\"aire_rw.txt\", \"w\")\n",
    "for j in aire_rw:   \n",
    "    f2.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f2.close()\n",
    "\n",
    "f3 = open (\"cftr_rw.txt\", \"w\")\n",
    "for j in cftr_rw:   \n",
    "    f3.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f3.close()\n",
    "\n",
    "f4 = open (\"rpsa_rw.txt\", \"w\")\n",
    "for j in rpsa_rw:   \n",
    "    f4.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f4.close()\n",
    "\n",
    "f5 = open (\"ada2_rw.txt\", \"w\")\n",
    "for j in ada2_rw:   \n",
    "    f5.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f5.close()\n",
    "\n",
    "f6 = open (\"gata2_rw.txt\", \"w\")\n",
    "for j in gata2_rw:   \n",
    "    f6.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f6.close()\n",
    "\n",
    "f7 = open (\"htt_rw.txt\", \"w\")\n",
    "for j in htt_rw:   \n",
    "    f7.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f7.close()\n",
    "\n",
    "f8 = open (\"ube3a_rw.txt\", \"w\")\n",
    "for j in ube3a_rw:   \n",
    "    f8.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f8.close()\n",
    "\n",
    "f9 = open (\"tyr_rw.txt\", \"w\")\n",
    "for j in tyr_rw:   \n",
    "    f9.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f9.close()\n",
    "\n",
    "f10 = open (\"serpinea1_rw.txt\", \"w\")\n",
    "for j in serpinea1_rw:   \n",
    "    f10.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f10.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "f1 = open (\"rictor_rw.txt\", \"w\")\n",
    "for j in rictor_rw:   \n",
    "    f1.write(j[0] + \"\\t\" + str(j[1]) + \"\\n\")\n",
    "f1.close()\n",
    "\n",
    "''' "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
